{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro\n",
    "\n",
    "In that notebook you will guided into a step by step pipeline that will generate a model able to produce embeddings on a specific dataset, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.6.0\n",
      "Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('Devices', tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are some definitions of folder paths that will be usefull later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "DATA_OUTPUT_DIR = os.path.join(DATA_DIR, 'output')\n",
    "MODELS_DIR = os.path.join(os.getcwd(), 'models')\n",
    "LOGS_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "INFERENCE_DIR = os.path.join(os.getcwd(), 'inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows preparation of the dataset consisting of convert data into npy format. In CHANNELS_MAP you could define which channels to use in case of multi-band images, an example will be provide with a notebook on the eurosat dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected files inside /home/thomas/Desktop/latent-space-explorer/generator/data/output directory\n",
      "Detected a metadata file\n",
      "Detected the same preparation config. Skipping preparation step...\n"
     ]
    }
   ],
   "source": [
    "from utils.preparation import preparation\n",
    "\n",
    "IMAGE_FORMAT = \"image\"\n",
    "CHANNELS_MAP = { \"r\": 0, \"g\": 0, \"b\": 0}\n",
    "preparation(DATA_DIR, IMAGE_FORMAT, CHANNELS_MAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the data preprocessing, note that the processing will be computed only when the training start. Here follows only the definition of the functions to be added to the tensorflow graph (in order to optimize the computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import tf_numpy_load, tf_preprocessing\n",
    "\n",
    "# Dataset\n",
    "pattern = os.path.join(DATA_OUTPUT_DIR, '*.npy')\n",
    "dataset = tf.data.Dataset.list_files(pattern)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda file: tf_numpy_load(file),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "NORMALIZATION_TYPE = \"default\"\n",
    "IMAGE_DIM = 28\n",
    "\n",
    "# Preprocessing\n",
    "dataset = dataset.map(\n",
    "    lambda image: tf_preprocessing(\n",
    "        image,\n",
    "        tf.constant(IMAGE_DIM, tf.uint16),\n",
    "        tf.constant(NORMALIZATION_TYPE, tf.string)\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(f'Dataset: {length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the training set is used to check if the model generalize or overfit. Here you could change the parameter SPLIT_THRESHOLD.\n",
    "\n",
    "Training and test are further splitted into batches in order to fit the available memory and optimize the training. The parameter to adjust here is BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8000\n",
      "Test set: 1999\n"
     ]
    }
   ],
   "source": [
    "SPLIT_THRESHOLD = 0.8\n",
    "index = round(length * SPLIT_THRESHOLD)\n",
    "train_set = dataset.take(index)\n",
    "test_set = dataset.skip(index + 1)\n",
    "\n",
    "print('Training set: {}'.format(\n",
    "    tf.data.experimental.cardinality(train_set).numpy()))\n",
    "print('Test set: {}'.format(\n",
    "    tf.data.experimental.cardinality(test_set).numpy()))\n",
    "\n",
    "train_set = train_set.cache()\n",
    "test_set = test_set.cache()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_set = train_set.shuffle(\n",
    "    len(train_set)).batch(BATCH_SIZE)\n",
    "test_set = test_set.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmentation step is useful to transform the data in a realistic way to increase the variability of the dataset. The operation is random and in order to let the model analyze the original data, AUGMENTATION_THRESHOLD parameter refer to the probability to perform the augmentation. The other parameters activate specific augmentations, that will be executed with a probability of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.augmentation import tf_augmentation\n",
    "\n",
    "AUGMENTATION_THRESHOLD = 0.7\n",
    "AUGMENTATION_FLIP_X = False\n",
    "AUGMENTATION_FLIP_Y = False\n",
    "AUGMENTATION_ROTATE = True\n",
    "AUGMENTATION_ROTATE_DEGREES = 5\n",
    "AUGMENTATION_SHIFT = True\n",
    "AUGMENTATION_SHIFT_PERCENTAGE = 10\n",
    "\n",
    "train_set = train_set.map(\n",
    "    lambda images:\n",
    "        tf.cond(\n",
    "            tf.random.uniform([], 0, 1) < AUGMENTATION_THRESHOLD,\n",
    "            lambda: tf_augmentation(\n",
    "                images,\n",
    "                tf.constant(AUGMENTATION_FLIP_X, tf.bool),\n",
    "                tf.constant(AUGMENTATION_FLIP_Y, tf.bool),\n",
    "                tf.constant(AUGMENTATION_ROTATE, tf.bool),\n",
    "                tf.constant(\n",
    "                    AUGMENTATION_ROTATE_DEGREES, tf.float32),\n",
    "                tf.constant(AUGMENTATION_SHIFT, tf.bool),\n",
    "                tf.constant(\n",
    "                    AUGMENTATION_SHIFT_PERCENTAGE, tf.float32)\n",
    "            ),\n",
    "            lambda: images\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_set = train_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_set = test_set.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows definition of the model, in that case a Convolutional AutoEncoder. \n",
    "\n",
    "The depth and width of the model is defined through FILTERS constant, each entry in the array is a convolutional layer with the specified amount of kernels/filters. The dimension of the latent vector (the real output of the autoencoder) is definable in the LATENT_DIM constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "down_sampling_0 (DownSamplin (None, 14, 14, 32)        1024      \n",
      "_________________________________________________________________\n",
      "down_sampling_1 (DownSamplin (None, 7, 7, 64)          18752     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 25096     \n",
      "=================================================================\n",
      "Total params: 44,872\n",
      "Trainable params: 44,680\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 3136)              28224     \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling_0 (UpSampling)   (None, 14, 14, 64)        37184     \n",
      "_________________________________________________________________\n",
      "up_sampling_1 (UpSampling)   (None, 28, 28, 32)        18592     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 3)         867       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 3)         0         \n",
      "=================================================================\n",
      "Total params: 84,867\n",
      "Trainable params: 84,675\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"cae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            (None, 8)                 44872     \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  84867     \n",
      "=================================================================\n",
      "Total params: 129,739\n",
      "Trainable params: 129,355\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from architectures.cae import CAE\n",
    "\n",
    "IMAGE_DIM = 28\n",
    "CHANNELS_NUM = 3 \n",
    "LATENT_DIM = 8\n",
    "FILTERS = [32, 64]\n",
    "\n",
    "model = CAE(\n",
    "    image_dim=IMAGE_DIM,\n",
    "    channels_num=CHANNELS_NUM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.001\n",
    "LOSS = \"MeanSquaredError\"\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss=LOSS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment folder creation is needed to save models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "EXPERIMENT_NAME = \"MNIST-ConvAutoEncoder-Small\"\n",
    "\n",
    "experiment_dir = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "experiment_dir = \"{}-{}\".format(experiment_dir, EXPERIMENT_NAME)\n",
    "\n",
    "# Create model dir\n",
    "model_dir = os.path.join(MODELS_DIR, experiment_dir)\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "# Set logger\n",
    "log_dir = os.path.join(LOGS_DIR, experiment_dir)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally training could be runned. Note that the preprocessing step is now performed and it could be a delay in the start of the training. So be patients.\n",
    "\n",
    "A mechanism for saving only the best model is implemented. After the middle of the EPOCHS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [11:12<09:43, 11.91s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [11:27<10:24, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [12:45<10:12, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [13:07<11:38, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [15:26<08:55, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [15:45<09:06, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [18:33<04:26, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [20:53<02:00, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [21:43<01:46, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [22:03<01:40, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [22:41<01:10, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/thomas/Desktop/latent-space-explorer/generator/models/20220322-180654-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [23:33<00:00, 14.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    # Train set\n",
    "    for batch, train_batch in enumerate(train_set):\n",
    "        model.train_step(train_batch)\n",
    "\n",
    "    # Test set\n",
    "    for batch, test_batch in enumerate(test_set):\n",
    "        model.test_step(test_batch)\n",
    "\n",
    "    # Save best model\n",
    "    if epoch > (EPOCHS / 2):\n",
    "        model.save_best_model(model_dir)\n",
    "\n",
    "    # Log\n",
    "    with summary_writer.as_default():\n",
    "        model.log(epoch, train_batch, test_batch)\n",
    "\n",
    "    # Reset losses\n",
    "    model.reset_losses_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to transform input images into embeddings and saves those well organized, ready to be uploaded into Nextcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestire il fatto di non avere il metadata.json\n",
    "# Lo creo dalle variabili?\n",
    "# Uso le variabili ma le sovrascrivo leggendole dal metadata?\n",
    "# Tipo:\n",
    "# IMAGE_DIM = 1\n",
    "# IMAGE_DIM = experiment_config['image']['dim']\n",
    "# Oppure gli faccio fare solo il training da 10 epoche su mnist\n",
    "# E poi a parte gli faccio fare la stessa pipeline usando il config_file\n",
    "\n",
    "experiment_dir = os.path.join(INFERENCE_DIR, experiment_dir)\n",
    "images_dir = os.path.join(experiment_dir, 'images')\n",
    "generated_dir = os.path.join(experiment_dir, 'generated')\n",
    "reductions_dir = os.path.join(experiment_dir, 'reductions')\n",
    "clusters_dir = os.path.join(experiment_dir, 'clusters')\n",
    "metadata_path = os.path.join(experiment_dir, 'metadata.json')\n",
    "embeddings_path = os.path.join(experiment_dir, 'embeddings.json')\n",
    "labels_path = os.path.join(experiment_dir, 'labels.json')\n",
    "\n",
    "os.makedirs(experiment_dir)\n",
    "os.makedirs(images_dir)\n",
    "os.makedirs(generated_dir)\n",
    "os.makedirs(reductions_dir)\n",
    "os.makedirs(clusters_dir)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
