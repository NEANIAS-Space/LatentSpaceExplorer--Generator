{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Generator - Train a model\n",
    "\n",
    "In order to generate a latent space for a given dataset, we need to first train a model capable of learn how to represent data.\n",
    "\n",
    "In that notebook you'll be guided into a step by step pipeline that will generate a model able to produce embeddings on a specific dataset, MNIST.\n",
    "\n",
    "We will use tensorflow as ML framework and an autoencoder as our model.\n",
    "\n",
    "<img src=\"autoencoder_schema.png\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.6.0\n",
      "Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('Devices', tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are some definitions of folder paths that will be usefull later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "DATA_OUTPUT_DIR = os.path.join(DATA_DIR, 'output')\n",
    "MODELS_DIR = os.path.join(os.getcwd(), 'models')\n",
    "LOGS_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "INFERENCE_DIR = os.path.join(os.getcwd(), 'inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Here follows preparation of the dataset, consisting of convert data into npy format.\n",
    "\n",
    "In CHANNELS_MAP you could define which channels to use in case of multi-band images, more detailed example will be provide with a notebook on the eurosat dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'astropy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3dc11f03200a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mIMAGE_FORMAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCHANNELS_MAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_FORMAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHANNELS_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/utils/preparation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFitsDataGenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFitsDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTifDataGenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTifDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/utils/converters/FitsDataGenerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mastropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'astropy'"
     ]
    }
   ],
   "source": [
    "from utils.preparation import preparation\n",
    "\n",
    "IMAGE_FORMAT = \"image\"\n",
    "CHANNELS_MAP = { \"r\": 0, \"g\": 0, \"b\": 0}\n",
    "preparation(DATA_DIR, IMAGE_FORMAT, CHANNELS_MAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The next step is the data preprocessing where resize and normalization are computed. \n",
    "\n",
    "NORMALIZATION_TYPE define how data are normalized, \"default\" value scale it between 0 and 1 by divide each pixel by 255. (it's the case of a general image). In the utils/preprocessing.py file you could define your own normalization function.\n",
    "\n",
    "IMAGE_DIM is the size of the image that will be used for training.\n",
    "\n",
    "The computation of that operation is postponed to the start of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cardinality: 10000\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import tf_numpy_load, tf_preprocessing\n",
    "\n",
    "# Dataset\n",
    "pattern = os.path.join(DATA_OUTPUT_DIR, '*.npy')\n",
    "dataset = tf.data.Dataset.list_files(pattern)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda file: tf_numpy_load(file),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "NORMALIZATION_TYPE = \"default\"\n",
    "IMAGE_DIM = 28\n",
    "\n",
    "# Preprocessing\n",
    "dataset = dataset.map(\n",
    "    lambda image: tf_preprocessing(\n",
    "        image,\n",
    "        tf.constant(IMAGE_DIM, tf.uint16),\n",
    "        tf.constant(NORMALIZATION_TYPE, tf.string)\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(f'Dataset cardinality: {length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "\n",
    "A subset of the training set is used to check if the model generalize or overfit. Here you could change the parameter SPLIT_THRESHOLD according to the portion of the dataset to use as training set.\n",
    "\n",
    "Training and test are further splitted into batches in order to fit the available memory and optimize the training. The parameter to adjust here is BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8000\n",
      "Test set: 1999\n"
     ]
    }
   ],
   "source": [
    "SPLIT_THRESHOLD = 0.8\n",
    "index = round(length * SPLIT_THRESHOLD)\n",
    "train_set = dataset.take(index)\n",
    "test_set = dataset.skip(index + 1)\n",
    "\n",
    "print('Training set: {}'.format(\n",
    "    tf.data.experimental.cardinality(train_set).numpy()))\n",
    "print('Test set: {}'.format(\n",
    "    tf.data.experimental.cardinality(test_set).numpy()))\n",
    "\n",
    "train_set = train_set.cache()\n",
    "test_set = test_set.cache()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_set = train_set.shuffle(\n",
    "    len(train_set)).batch(BATCH_SIZE)\n",
    "test_set = test_set.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "The augmentation step is useful to transform the data, in a realistic way, to increase the variability of the dataset. The operation is random and in order to let the model analyze the original data, AUGMENTATION_THRESHOLD parameter activate/deactivate all augmentation with a probability. The other parameters activate specific augmentations, that will be executed with a probability of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.augmentation import tf_augmentation\n",
    "\n",
    "AUGMENTATION_THRESHOLD = 0.7\n",
    "AUGMENTATION_FLIP_X = False\n",
    "AUGMENTATION_FLIP_Y = False\n",
    "AUGMENTATION_ROTATE = True\n",
    "AUGMENTATION_ROTATE_DEGREES = 5\n",
    "AUGMENTATION_SHIFT = True\n",
    "AUGMENTATION_SHIFT_PERCENTAGE = 10\n",
    "\n",
    "train_set = train_set.map(\n",
    "    lambda images:\n",
    "        tf.cond(\n",
    "            tf.random.uniform([], 0, 1) < AUGMENTATION_THRESHOLD,\n",
    "            lambda: tf_augmentation(\n",
    "                images,\n",
    "                tf.constant(AUGMENTATION_FLIP_X, tf.bool),\n",
    "                tf.constant(AUGMENTATION_FLIP_Y, tf.bool),\n",
    "                tf.constant(AUGMENTATION_ROTATE, tf.bool),\n",
    "                tf.constant(\n",
    "                    AUGMENTATION_ROTATE_DEGREES, tf.float32),\n",
    "                tf.constant(AUGMENTATION_SHIFT, tf.bool),\n",
    "                tf.constant(\n",
    "                    AUGMENTATION_SHIFT_PERCENTAGE, tf.float32)\n",
    "            ),\n",
    "            lambda: images\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_set = train_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_set = test_set.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows definition of the model, in that case a Convolutional AutoEncoder. \n",
    "\n",
    "The depth and width of the model is defined through FILTERS constant, each entry in the array is a convolutional layer with the specified amount of kernels/filters. The dimension of the latent vector (the real output of the autoencoder) is definable in the LATENT_DIM constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "down_sampling_0 (DownSamplin (None, 14, 14, 32)        448       \n",
      "_________________________________________________________________\n",
      "down_sampling_1 (DownSamplin (None, 7, 7, 64)          18752     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 25096     \n",
      "=================================================================\n",
      "Total params: 44,296\n",
      "Trainable params: 44,104\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3136)              28224     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling_0 (UpSampling)   (None, 14, 14, 64)        37184     \n",
      "_________________________________________________________________\n",
      "up_sampling_1 (UpSampling)   (None, 28, 28, 32)        18592     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 84,289\n",
      "Trainable params: 84,097\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"cae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            (None, 8)                 44296     \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  84289     \n",
      "=================================================================\n",
      "Total params: 128,585\n",
      "Trainable params: 128,201\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from architectures.cae import CAE\n",
    "\n",
    "IMAGE_DIM = 28\n",
    "CHANNELS_NUM = len(set(CHANNELS_MAP.values()))\n",
    "LATENT_DIM = 8\n",
    "FILTERS = [32, 64]\n",
    "\n",
    "model = CAE(\n",
    "    image_dim=IMAGE_DIM,\n",
    "    channels_num=CHANNELS_NUM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.001\n",
    "LOSS = \"MeanSquaredError\"\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss=LOSS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment folder creation is needed to save models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "EXPERIMENT_NAME = \"MNIST-ConvAutoEncoder-Small\"\n",
    "\n",
    "experiment_dir = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "experiment_dir = \"{}-{}\".format(experiment_dir, EXPERIMENT_NAME)\n",
    "\n",
    "# Create model dir\n",
    "model_dir = os.path.join(MODELS_DIR, experiment_dir)\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "# Set logger\n",
    "log_dir = os.path.join(LOGS_DIR, experiment_dir)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save details about the training process, a metadata.json file is created. That configuration is useful in inference phase.\n",
    "\n",
    "Usually metadata are picked from configuration file: config.json. But for sake of understandability we create it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "CONFIG = {}\n",
    "\n",
    "\n",
    "CONFIG['name'] = EXPERIMENT_NAME\n",
    "CONFIG['image'] = {}\n",
    "CONFIG['image']['format'] = IMAGE_FORMAT\n",
    "CONFIG['image']['dim'] = IMAGE_DIM\n",
    "CONFIG['image']['channels'] = {}\n",
    "CONFIG['image']['channels']['map'] = CHANNELS_MAP\n",
    "CONFIG['image']['channels_map'] = CHANNELS_MAP\n",
    "CONFIG['dataset'] = {}\n",
    "CONFIG['dataset']['split_threshold'] = SPLIT_THRESHOLD\n",
    "CONFIG['preprocessing'] = {}\n",
    "CONFIG['preprocessing']['normalization_type'] = NORMALIZATION_TYPE\n",
    "CONFIG['augmentation'] = {}\n",
    "CONFIG['augmentation']['threshold'] = AUGMENTATION_THRESHOLD\n",
    "CONFIG['augmentation']['flip_x'] = AUGMENTATION_FLIP_X\n",
    "CONFIG['augmentation']['flip_y'] = AUGMENTATION_FLIP_Y\n",
    "CONFIG['augmentation']['rotate'] = {}\n",
    "CONFIG['augmentation']['rotate']['enabled'] = AUGMENTATION_ROTATE\n",
    "CONFIG['augmentation']['rotate']['degrees'] = AUGMENTATION_ROTATE_DEGREES\n",
    "CONFIG['augmentation']['shift'] = {}\n",
    "CONFIG['augmentation']['shift']['enabled'] = AUGMENTATION_SHIFT\n",
    "CONFIG['augmentation']['shift']['percentage'] = AUGMENTATION_SHIFT_PERCENTAGE\n",
    "CONFIG['architecture'] = {}\n",
    "CONFIG['architecture']['name'] = \"cae\" # convolutional autoencoder. Useful to run main.py script in order to choose the architecture.\n",
    "CONFIG['architecture']['filters'] = FILTERS\n",
    "CONFIG['architecture']['latent_dim'] = LATENT_DIM\n",
    "CONFIG['training'] = {}\n",
    "CONFIG['training']['epochs'] = 10 # number of epochs, defined later\n",
    "CONFIG['training']['batch_size'] = BATCH_SIZE\n",
    "CONFIG['training']['optimizer'] = {}\n",
    "CONFIG['training']['optimizer']['name'] = OPTIMIZER\n",
    "CONFIG['training']['optimizer']['learning_rate'] = LEARNING_RATE\n",
    "CONFIG['training']['loss'] = LOSS\n",
    "\n",
    "# Save experiement config\n",
    "experiment_config = os.path.join(\n",
    "    MODELS_DIR, experiment_dir, 'config.json')\n",
    "with open(experiment_config, 'w+') as f:\n",
    "    json.dump(CONFIG, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the training step could be executed. Note that, before start the training, the preprocessing step defined above is executed, and so there could be a delay in the start of the training. Be patients.\n",
    "\n",
    "A mechanism for saving only the best model is implemented. After the middle of the EPOCHS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:01<00:27,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/models/20220323-111325-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|███████   | 7/10 [01:09<00:21,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/models/20220323-111325-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████  | 8/10 [01:17<00:15,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/models/20220323-111325-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|█████████ | 9/10 [01:25<00:07,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/models/20220323-111325-MNIST-ConvAutoEncoder-Small/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:33<00:00,  9.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "    # Train set\n",
    "    for batch, train_batch in enumerate(train_set):\n",
    "        model.train_step(train_batch)\n",
    "\n",
    "    # Test set\n",
    "    for batch, test_batch in enumerate(test_set):\n",
    "        model.test_step(test_batch)\n",
    "\n",
    "    # Save best model\n",
    "    if epoch > (EPOCHS / 2):\n",
    "        model.save_best_model(model_dir)\n",
    "\n",
    "    # Log\n",
    "    with summary_writer.as_default():\n",
    "        model.log(epoch, train_batch, test_batch)\n",
    "\n",
    "    # Reset losses\n",
    "    model.reset_losses_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
